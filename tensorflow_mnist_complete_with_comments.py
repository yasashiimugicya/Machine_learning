# -*- coding: utf-8 -*-
"""TensorFlow_MNIST_complete_with_comments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JOXpEc6dSVzTLDYPryj_2M6AXSKJCkk4

# MNISTによる分類問題

## ライブラリのインポート
"""

import numpy as np
import tensorflow as tf

# tensorflow-datasetsモジュールを使ってMNISTデータセットをインポートしていきます。まだインストールしていない場合は以下のコマンドでインストールしましょう。
# pip install tensorflow-datasets 
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# このデータセットは、C:\Users\*USERNAME*\tensorflow_datasets\...に保存されます。

"""## データの読み込みと前処理

"""

# tfds.loadを使ってデータセットを読み込んでいきます 
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True をすることによって、データに関する情報を得ることができるようになります。

# as_supervised=Trueをすることによって、データを入力とターゲットの二つののタプル形式で作成することができます 

# 訓練用データとテスト用データをそれぞれの変数に代入します
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# ここから検証用データセットを作っていきます。まずは、検証用データに割り当てる割合を決めます。
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# 数字を整数に変換していきます（既に整数になっている場合も念のため行います）
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# テストデータの数も変数に入れていきます
num_test_samples = mnist_info.splits['test'].num_examples
# 数字を整数に変換していきます
num_test_samples = tf.cast(num_test_samples, tf.int64)


# 元の入力データの範囲である0から255を0と1に変えていきます
# 今回は、変換するための関数を作成していきます
def scale(image, label):
    # 数字が小数であることを確認します
    image = tf.cast(image, tf.float32)
    # 0と1の範囲におさめるため、255で割っていきます 
    image /= 255.

    return image, label


# mapメソッドを使ってデータの変換を行っていきます
scaled_train_and_validation_data = mnist_train.map(scale)

# テストデータに関しても同様の変換を行っていきます
test_data = mnist_test.map(scale)

#データをシャッフルするためのバッファのサイズを決めていきます
BUFFER_SIZE = 10000

# シャッフルメソッドを使ってデータをシャッフルしていきます
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# takeメソッドを使ってシャッフルした検証用データを変数に代入していきます
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# skipメソッドを使って訓練用データセットを変数に代入します
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# batchの大きさを定義します
BATCH_SIZE = 100

# 訓練データをバッチの数毎に分けていきます
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# テストデータに関してもバッチ毎に分けていきます
test_data = test_data.batch(num_test_samples)


# バリデーションデータセットに関し、入力とターゲットそれぞれの変数を作成していきます
validation_inputs, validation_targets = next(iter(validation_data))

"""## モデルの作成

### モデルの作成
"""

input_size = 784
output_size = 10
# 隠れ層のユニットの数を定義します
hidden_layer_size = 50
    
# モデルの定義を進めていきます
model = tf.keras.Sequential([
    
    # 入力層の定義をしていきます
    # もとの行列のデータをベクトルに変換します
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # 入力層
    
    # tf.keras.layers.Dense は線形結合をするための計算を行います。つまり、入力と重みの積にバイアスを追加する形です
    # 引数として、ここでは隠れ層の数と活性化関数を指定します
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1番目の隠れ層
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2番目の隠れ層
    
    # 最後の層の活性関数はソフトマックスとします
    tf.keras.layers.Dense(output_size, activation='softmax') # 出力層
])

"""### 最適化アルゴリズムと損失関数の選択"""

# 最適化アルゴリズムはadamを、損失関数はクロスエントロピーを使っていきます
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""### 訓練

"""

# 繰り返しの回数を決めます
NUM_EPOCHS = 5

# モデルに対してデータを入れて演算を行っていきます
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)

"""## モデルのテスト"""

test_loss, test_accuracy = model.evaluate(test_data)

# 出力結果の見た目を整えます
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))