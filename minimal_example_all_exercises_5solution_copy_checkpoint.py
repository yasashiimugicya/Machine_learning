# -*- coding: utf-8 -*-
"""Minimal_example_All_Exercises - 5solution  - Copy-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-xEs6aFHYDHVLzOBLQXFbigm_S6ieQ4

# 単線形回帰

6. f(x,z) = 13*xs + 7*zs - 12にしてモデルを実行していきましょう

## 解答

考察:
1. ほとんどの結果は同じであることが分かります

### ライブラリのインポート
"""

import numpy as np

# matplotlib と mpl_toolkits は必須ではないですが、グラフを書く際に必要となります。  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

"""### 訓練させるためのランダムな入力データの作成"""

# まずは作成したいデータの数を入れた変数を作成します
observations = 1000

# 入力の変数は2つです
# 今回はそれをxとzとします
# その値は一様分布を使ってランダムに決めていきます。uniformメソッドの引数は3つです(low, high, size).
# xs と zs のサイズは1000 x 1です
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# 2つの変数を一つの行列にします 
# column_stackは Numpy のメソッドであり、2つのベクトルを組み合わせて行列にします
inputs = np.column_stack((xs,zs))

# 行列のサイズは n x kとなり、 n はデータの数を示しており、 k は変数の数を示しています。つまり、今回は1000 x 2となります。
print (inputs.shape)



"""### ターゲットの作成"""

# ターゲットとする値にはランダムなノイズを加えます f(x,z) = 2x - 3z + 5 + <ノイズ>
noise = np.random.uniform(-1, 1, (observations,1))

# f(x,z) = 13x + 7z - 12 + noise の式を使ってターゲットを作成します
# ここでは、重みはそれぞれ2と-3となり、バイアスは5です
targets = 13*xs + 7*zs - 12 + noise

# 念の為、ターゲットのサイズを確認しておきましょう。今回の場合はn x mであり、mは出力の数ということから1000 x 1となります
print (targets.shape)

"""### 訓練データのプロット"""

# 3D plotを使うためにはオブジェクトはある形状でなければいけないことから、reshapeメソッドを使っていきます
targets = targets.reshape(observations,)

# matplotlib.pyplotを使って散布図を作成していきます

# 変数を宣言します
fig = plt.figure()

# 3dのグラフを作成するためのメソッドを書いていきます
ax = fig.add_subplot(111, projection='3d')

# 軸を選びます
ax.plot(xs, zs, targets)

# ラベルに名前を付けます
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# azimの値を変えることで、グラフをどこから見るのかという点を変更することができます
ax.view_init(azim=100)

# 実際に散布図を表示していきます 
plt.show()

# ターゲットの形状を元に戻していきます
targets = targets.reshape(observations,1)

"""### 変数の初期化"""

# 重みとバイアスを、ある範囲でランダムに決めていきます
# その範囲を決める変数としてinit_rangeを定義します
init_range = 0.1

# 重みのサイズは k x mであり k は入力の数、 m は出力の数です
# 今回の場合においては、重みの行列は2x1となります。
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# バイアスのサイズは1であり、スカラーとなります
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#初期化の結果を確認してみましょう
print (weights)
print (biases)

"""### 学習率の設定"""

# 今回の例では、学習率が0.02だと非常に良く機能します。
learning_rate = 0.02

"""### モデルの訓練"""

# 今回は繰り返しの回数は100回とします
for i in range (100):
    
    # y = xw + b の線形モデルの等式です
    outputs = np.dot(inputs,weights) + biases
    # デルタは出力とターゲットの差を示しています
    # ここではデルタは1000x1のベクトルです
    deltas = outputs - targets
        
    # L2ノルム損失を使って損失の計算をしていきます。
    # また、コードを一般化させるためにデータの数で割っています。
    loss = np.sum(deltas ** 2)  / observations
    deltas_signs = np.sign(deltas)
    deltas_signs[deltas_signs == 0] = 1
    
    # それぞれの繰り返しにおける損失関数の値を出力します.
    print (loss)
    
    # 損失関数の場合と同じように、デルタに関してもデータの数で割っていきます
    deltas_scaled = deltas / observations
    
    # 勾配降下法のアルゴリズムを適用していきます
    # 重みは2x1, 学習率は1x1 (scalar), 入力は1000x2, デルタは1000x1となります
    # 演算を適切に行うために、入力の転置を取ります
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)

"""### 重みとバイアスの出力"""

# 重みとバイアスの値が適切かどうか確認しましょう。（重みは2と-3、バイアスは5でした。）
print (weights, biases)

"""### 出力とターゲットの図示
出力とターゲットを散布図に示し、もしその直線が45度に近ければ、正しい（出力とターゲットの値が同じに近い）ということができます。
"""

plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()